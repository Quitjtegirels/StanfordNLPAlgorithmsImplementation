# -*- coding: utf-8 -*-
"""Nlpstanford.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1guPl3xXKXwr5Vd5XJwjlDbX1Z8zn4tbW
"""

#This will be the complete library for the algorithms in the book Speech and Language Processing by Jurafsky and Martin
# Do not forget run before go


# Libraries
import re
import numpy as np
import random
import os
from collections import defaultdict

"""\Chapter 2 (Mini ELIZA, and Minimum Edit Distance Algorithm)


"""

# Chapter 2 (Mini ELIZA, and Minimum Edit Distance Algorithm)


#Mini eliza

def miniELIZA(text):
  pattern1 = 'I am (sad|depressed)'
  pattern2 ='.* ?all .*'
  pattern3 = '.* always .*'
  x = re.findall(pattern1,text)
  y = re.findall(pattern2,text)
  z = re.findall(pattern3,text)
  if len(x) != 0:
    answer = re.sub(pattern1,'But it is normal to be sad',text)
  if len(y) != 0:
    answer = re.sub(pattern2,'Do not over generalize',text)
  if len(z) != 0:
    answer = re.sub(pattern3,'Can you tell me what you mean by always?',text)
  return answer
print(miniELIZA('all men are rude'))

# Levensthein distance algorithm

def levenshteinDistance(a,b):
  if len(a) == 0:
    return len(b)
  elif len(b) == 0:
    return len(a)
  elif a[0] == b[0]:
    return levenshteinDistance(a[1:],b[1:])
  else:
    x = levenshteinDistance(a[1:],b)
    y = levenshteinDistance(a,b[1:])
    z = levenshteinDistance(a[1:],b[1:])
    return 1+min(x,y,z)

# Levensthein distance dynamic programming

def levenshteinMatrix(a,b):
  m = len(a)
  n = len(b)
  dm = [[0 for x in range(n+1)] for x in range (m+1)]
  for i in range(m+1):
    for j in range(n+1):
      if i == 0:
        dm[i][j] = j
      elif j == 0:
        dm[i][j] = i
      elif a[i-1] == b[j-1]:
        dm[i][j] = dm[i-1][j-1]
      else:
        dm[i][j] = 1 + min(dm[i-1][j],
                           dm[i][j-1],
                           dm[i-1][j-1])
  return dm[m][n]

"""Chapter 3: N-gram Language Models"""

# Chapter 3: N-gram Language Models


# make ngrams, then given a context calculate the probability of a word ocurring

class NgramLanguageModel:
  def __init__(self,n,k=1):
    self.n = n
    self.k = k
    self.ngrams = defaultdict(int)
    self.contexts = defaultdict(int)
    self.vocabulary = set()
    self.vocab_size = 0

  # train using MLE, corpus is list in which sentences are element of list.

  def train(self,corpus):
    for sentence in corpus:
      tokens = sentence.split()
      for token in tokens:
        self.vocabulary.add(token)
      for j in range(len(tokens)- self.n + 1):
        ngram = ' '.join(tokens[j:j+self.n])
        context = ' '.join(tokens[j:j+self.n-1])
        self.ngrams[ngram] += 1
        self.contexts[context] += 1
    self.vocab_size = len(self.vocabulary)
  def prob(self,word,context):
    context = ' '.join(context.split()[-(self.n-1):])
    ngram = context + ' ' + word
    if context not in self.contexts:
      return 1/self.vocab_size
    if word not in self.vocabulary:
      return self.k/(self.contexts[context] + self.k * self.vocab_size)
    return (self.ngrams[ngram] + self.k) / (self.contexts[context] + self.k * self.vocab_size)
  def sample(self,context): #Sampler
    context = ' '.join(context.split()[-(self.n-1):])
    if context not in self.contexts:
      return None
    candidates = []
    probabilities = []
    for word in self.vocabulary:
      prob = self.prob(word,context)
      candidates.append(word)
      probabilities.append(prob)
    sampled_word = random.choices(candidates,weights = probabilities, k=1)[0]
    return sampled_word
  def generate_sentence(self, max_length = 20, start_context = None):
    if start_context is None:
      start_context = ' '.join(['<s>'] * (self.n-1))
    sentence = start_context
    for _ in range(max_length):
      sampled_word = self.sample(sentence)
      if sampled_word is None or sampled_word == '<s>':
        break
      sentence += ' ' + sampled_word
    return sentence

# A trial

corpus = '''Odalarda ışıksızım katıksızım viraneyim
  Seni sensiz duvarlara yazan benim divaneyim
  Kanım aksın ki terk etmem seni
Peşindeyim yar
Ellerimsin gözlerimsin inanmazsın yar
Ben perişan günlerim dar anlamazsın yar
Bir ömür bu zindanlarda ellerimsin gözlerimsin
Mahkumum sana
Davalı ben davacı ben yorgunum bu celselerden
Dargınım sana
Posta posta hatıralar oltalarda yar
Ben perişan günlerim dar anlamazsın yar
Odalarda ışıksızım katıksızım viraneyim
Seni sensiz duvarlara yazan benim divaneyim
Kanım aksın ki terk etmem seni
Peşindeyim yar
Ellerimsin gözlerimsin inanmazsın yar
Ben perişan günlerim dar anlamazsın yar
Bir ömür bu zindanlarda ellerimsin gözlerimsin
Mahkumum sana
Davalı ben davacı ben yorgunum bu celselerden
Dargınım sana
Posta posta hatırlar oltalarda yar
Ben perişan günlerim dar anlamazsın yar
Odalarda ışıksızım katıksızım viraneyim
Seni sensiz duvarlara yazan benim divaneyim
Kanım aksın ki terk etmem seni
Peşindeyim yar'''
processed_corpus = corpus.lower()
processed_corpus2 = processed_corpus.split('\n')
model = NgramLanguageModel(n=2,k=1)
model.train(processed_corpus2)

"""Chapter 4: Naive Bayes Classifier and Sentiment Classification"""

# Chapter 4: Naive Bayes Classifier and Sentiment Classification


# inputs are lists containing strings.
class NaiveBayesClassifierMY:
  def __init__(self):
    self.priors = {}
    self.likelihoods = {}
  def preprocess(self,X):
    vocabulary = set()
    for document in X:
      vocabulary.update(document.split())
    return list(vocabulary)
  def train(self,X,y):
    vocabulary = self.preprocess(X)
    #First prior
    sample_size_p = len(y)
    for label in set(y):
      self.priors[label] = sum(1 for label_value in y if label_value == label)/sample_size_p

    #likelihoods
    for label in set(y):
      label_index = [i for i, label_value in enumerate(y) if label_value == label]
      label_X = [X[i] for i in label_index]
      sample_size_l = len(label_X)
      self.likelihoods[label] = {}
      for word in vocabulary:
        word_counts = sum(document.split().count(word) for document in label_X)
        self.likelihoods[label][word] = word_counts / sample_size_l

  def predict(self,X):
    predictions = []
    for document in X:
      words = document.split()
      max_posterior_prob = -(np.inf)
      posterior_prob = 0
      predicted_label = None
      for label in self.priors:
        posterior_prob = np.log(self.priors[label])
        for word in words:
          if word in self.likelihoods[label]:
            posterior_prob += np.log(self.likelihoods[label][word])
        if posterior_prob > max_posterior_prob:
          max_posterior_prob = posterior_prob
          predicted_label = label
      predictions.append(predicted_label)
    return predictions

# Logistic Regression

class LogisticRegression:
  def __init__(self, learning_rate = 0.01, num_iterations = 1000):
    self.learning_rate = learning_rate
    self.num_iterations = num_iterations
    self.weight = None
    self.bias = None


  def sigmoid(self,z):
    return 1/(1-np.exp(-z))


  def fit(self, X, y):
    num_samples, num_features = X.shape
    self.weights = np.zeros(num_features)
    self.bias = 0

    # Gradiant descent

    for i in range(self.num_iterations):
      linear_model = np.dot(X,self.weights) + self.bias
      y_predicted = self.sigmoid(linear_model)

      # Computing gradiants

      dw = (1/num_samples) * np.dot(X.T, (y_predicted - y))
      db = (1/num_samples) * np.dot(y_predicted-y)

      # Parameter update

      self.weights = self.learning_rate*dw
      self.bias = self.learning_rate-db
  def predict(self,X):
    linear_model = np.dot(X,self.weights) + self.bias
    y_predicted = self.sigmoid(linear_model)
    y_predicted_class = [1 if i>0.5 else 0 for i in y_predicted]
    return np.array(y_predicted_class)


  def scaling_input(self,X,feature):
    num_samples, num_features = X.shape
    specific_feature = X[:,feature]
    mean_feature = (1/num_samples)*(np.sum(specific_feature,axis = 0))
    st_dev = np.sqrt((1/num_samples)*(np.sum(specific_feature,axis = 0)-(mean_feature*num_samples)))
    return (specific_feature - mean_feature)/st_dev

# Neural Networks


# A perceptron of 2 inputs, 3 weights, and one output. Trying to find the truth value given of 'or' operator that takes x1 and x2 as its disjuncts

def perceptron(x1,x2,y):
  lr = 1
  bias = 1
  weights = [random.random(),random.random(),random.random()]
  outputP = x1*weights[0] + x2*weights[1] + bias*weights[3]

 #Heaviside function as the activation function

  if outputP > 1:
    outputP = 1
  else:
    outputP = 0
  error = y-outputP
  weights[0] += error * x1 * lr
  weights[1] += error * x2 * lr
  weights[2] += error * bias * lr